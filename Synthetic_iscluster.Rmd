---
title: "exemple data"
output: html_document
---

usefull comands for python

repl_python()
import random

random.seed(212)
features, target = make_blobs(n_samples = 50, n_features = 5, centers = 4, cluster_std = 0.65, shuffle = True)

library(readr)
library(RCurl)

url <- "https://ucffc7498f465ffb5241124780f8.dl.dropboxusercontent.com/cd/0/get/Aq2g01WwJ4AfnoJujJqc5iwp7QioeCZFAw_5mRb_n8koyVJN5q8qG2diOeB72Ce3X_ndUGVoYwCJ8gD64pMOUiTXmQs2rZsCRvRS2Wl0vtn-eGP4SD5tKVrgRBmROdgMa9E/file?dl=1#"


download.file(url, destfile = "10_21_data.RData")

load("10_21_data.RData")



```{r}
#function: remove zero varience data
Return.Zero.Variance.Rows = function(mat){
  rowVars <- apply(mat, 1, var);
  isConstant <-(rowVars <= 0.01);
  return(isConstant);
}
 
#function: Keep columns as specified in colList (logicals). 
Keep.Rows = function(mat, rowList) { 
	return(mat[rowList,]); 
}

#keep common patients

keep.common <- function(data){
  common.patient <- intersect(colnames(data[[1]]), colnames(data[[2]]))
  common.patient <- intersect(common.patient, colnames(data[[3]]))
  data[[1]] <- data[[1]][,common.patient]
  data[[2]] <- data[[2]][,common.patient]
  data[[3]] <- data[[3]][,common.patient]
  return(data)
}


#function that remove rows that have variance < than "variance" variable
#arguments: data - list of matrix
#           variance - variance threshold

remove.Zero.varience <- function(data, variance){
  
  #remove zero varience data
  Return.Zero.Variance.Rows = function(mat, variance){
    rowVars <- apply(mat, 1, var);
    isConstant <-(rowVars <= variance);
    return(isConstant);
  }
  #Keep columns as specified in colList (logicals). 
  Keep.Rows = function(mat, rowList) { 
  	return(mat[rowList,]); 
  }
  
  for (view in 1:length(data)){
    view
    zerorowlist <- Return.Zero.Variance.Rows(data[[view]], variance);
    data[[view]]<-Keep.Rows(data[[view]], !zerorowlist);
  }
  return(data)
}



makeSimData <- function (n_views = 3, n_features = 100, n_samples = 50, n_factors = 5, 
    likelihood = "gaussian", DZ = NULL)
{
    if (!all(likelihood %in% c("gaussian", "bernoulli", "poisson"))) 
        stop("Liklihood not implemented: Use either gaussian, bernoulli or poisson")
    if (length(likelihood) == 1) 
        likelihood <- rep(likelihood, n_views)
    if (!length(likelihood) == n_views) 
        stop("Likelihood needs to be a single string or matching the number of views!")
    #latent factors
    
    if(length(DZ)==0){
      Z <- matrix(rnorm(n_factors * n_samples, 0, 1), nrow = n_samples, ncol = n_factors)
    }else{
      Z <- DZ
    }
    
    
    theta <- 0.5
    alpha <- sapply(1:n_factors, function(fc) {
        active_vw <- sample(1:n_views, 1)
        alpha_fc <- sample(c(1, 1000), n_views, replace = TRUE)
        if (all(alpha_fc == 1000)) 
            alpha_fc[active_vw] <- 1
        alpha_fc
    })
    alpha <- t(alpha)
    
    #S- lista matrix de 1 e 0
    S <- lapply(1:n_views, function(vw) matrix(rbinom(n_features * 
        n_factors, 1, theta), nrow = n_features, ncol = n_factors))
    
    #list of matrix W with normal distribuition, mean=0, and sd = 1/alpha
    W <- lapply(1:n_views, function(vw) sapply(1:n_factors, function(fc) rnorm(n_features, 
        0, sqrt(1/alpha[fc, vw]))))
    tau <- 10
    
    
    mu <- lapply(1:n_views, function(vw) Z %*% t(S[[vw]] * W[[vw]]))
    
    
    data <- lapply(1:n_views, function(vw) {
        lk <- likelihood[vw]
        if (lk == "gaussian") {
            dd <- t(mu[[vw]] + rnorm(length(mu[[vw]]), 0, sqrt(1/tau)))
        }
        else if (lk == "poisson") {
            term <- log(1 + exp(mu[[vw]]))
            dd <- t(apply(term, 2, function(tt) rpois(length(tt), 
                tt)))
        }
        else if (lk == "bernoulli") {
            term <- 1/(1 + exp(-mu[[vw]]))
            dd <- t(apply(term, 2, function(tt) rbinom(length(tt), 
                1, tt)))
        }
        colnames(dd) <- paste0("sample_", 1:ncol(dd))
        rownames(dd) <- paste0("feature", 1:nrow(dd))
        dd
    })
    names(data) <- paste0("view_", 1:n_views)
    return(data)
}

```





python for make cluster data
```{r}

library(reticulate)
sklearn <- import("sklearn.datasets")  
py_available(TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)

```

Z:
number_of_test = 8
samples_size = [50,50,50,50,50,50,50,50]
latents_size = [2,2,2,2,2,2,2,2]
cluster_size = [3,4,5,6,5,3,4,5,6]
cluster_std = [1, 1, 1, 1, 0.5, 0.5,0.5,0.5]

Z:
number_of_test = 8
samples_size = [50,100,500]
latents_size = [2,5,10,15,30,50,100]
cluster_size = [2,3,4,5,6,7,8,9]
cluster_std = [0.25,0.5,0.75,1,1.25,1.5,2]



Y:

```{r}

samples.size <- c(100)
features.size <- c(20,50,100,500,1000)
latents.size <- c(3,5,10,15)
cluster.size <- c(2,3,4,5,6,7,8,9)
views.size <- c(3)




```



repl_python()
```{python}


from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import random
import math
import numpy as np

random.seed(212)
# Make features and targets with 500 samples,

samples_size = [100]
latents_size = [3,5,10,15]
cluster_size = [2,3,4,5,6,7,8,9]
cluster_std = [0.25,0.5,0.75,1,1.25,1.5,1.75,2]



samples_index = np.full((len(samples_size),len(latents_size),len(cluster_size),len(cluster_std)), 0)
index = 0
for i in range(0, len(samples_size)):
  for j in range(0, len(latents_size)):
    for k in range(0, len(cluster_size)):
      for l in range(0, len(cluster_std)):
        samples_index[i,j,k,l] = index
        index = index + 1

test_samples = list()
test_labels = list()
for i in range(0, len(samples_size)):
  for j in range(0, len(latents_size)):
    for k in range(0, len(cluster_size)):
      for l in range(0, len(cluster_std)):
        data, label = make_blobs(n_samples = samples_size[i], n_features = latents_size[j], centers = cluster_size[k], cluster_std = cluster_std[l], shuffle = True,   random_state=0)
        test_samples.append(data)
        test_labels.append(label)




```


get python variables
```{r}
library(MOFA)
library(clusterCrit)
library(factoextra)
library(NbClust)


latent <- py$test_samples
labels <- py$test_labels
Z.index <- py$samples_index

#normalization of list of Zt
for (l in 1:length(latent)){
  latent[[l]]<- scale(latent[[l]])
}

#data transformation from Z to Y
set.seed(1234)

samples.size <- c(50,100,500)
features.size <- c(20,50,100,500,1000,5000,10000,20000,50000)
latents.size <- c(2,5,10,15,30,50,100)
cluster.size <- c(2,3,4,5,6,7,8,9)
views.size <- c(2,3,4,5)
cluster.std <- c(0.25,0.5,0.75,1,1.25,1.5,2)

Y.index <- array(0, dim=c(length(samples.size),length(latents.size),length(cluster.size),length(cluster.std),length(views.size),length(features.size)))
Y.samples <- list()
Y.labels <- list()

index <- 1
for(i in 1:length(samples.size)){
  for(j in 1:length(latents.size)){
    for(k in 1:length(cluster.size)){
      for(l in 1:length(cluster.std)){
        for(m in 2:2){            #length(views.size)
          for(n in 1:4){      #length(features.size)
            if(features.size[[n]] > latents.size[[j]]){
              Y.samples[[index]] <- makeSimData(n_views = views.size[[m]], n_features = features.size[[n]], n_samples = samples.size[[i]], n_factors = latents.size[[j]], likelihood = "gaussian", DZ = latent[[Z.index[i,j,k,l]+1]])
              Y.index[i,j,k,l,m,n] <- index
              Y.labels[[index]] <- labels[[Z.index[i,j,k,l]+1]]
              index <- index + 1 
              print(index)
            }
          }
        }
      }
    }
  }
}

sl <- object.size(Y.samples)


#filtra variaveis com varianca inferior a 1%
processed.data <- list()
for (l in 1:length(simulated.data)){
  processed.data[[l]] <- remove.Zero.varience(simulated.data[[l]], 0.01)
}
```


```{r}

Mofa.cluster.index <- vector("list", length = length(Y.samples))
Mofa.real.index <- vector("list", length = length(Y.samples))
mofa.result <- vector("list", length = length(Y.samples))


index <- 1
for(i in 1:1){       #length(samples.size)
  for(j in 1:length(latents.size)){
    for(k in 1:length(cluster.size)){
      for(l in 1:length(cluster.std)){
        for(m in 2:2){            #length(views.size)
          for(n in 1:4){      #length(features.size)
            if(features.size[[n]] > latents.size[[j]]){
  
  data <- Y.samples[[Y.index[i,j,k,l,m,n]+1]]
  MOFAobject <- createMOFAobject(data)
  MOFAobject
  plotDataOverview(MOFAobject)
  
  TrainOptions <- getDefaultTrainOptions()
  TrainOptions
  ModelOptions <- getDefaultModelOptions(MOFAobject)
  ModelOptions$likelihood <- "gaussian"
  DataOptions <- getDefaultDataOptions()
  DataOptions
  TrainOptions$DropFactorThreshold <- 0.01
  
  n_inits <- 10
  MOFAlist <- lapply(seq_len(n_inits), function(it) {
    TrainOptions$seed <- 2018+ it
    MOFAobject <- prepareMOFA(MOFAobject, DataOptions = DataOptions, ModelOptions = ModelOptions, TrainOptions = TrainOptions)
    
    runMOFA(MOFAobject)
  })

  compareModels(MOFAlist)
  compareFactors(MOFAlist)
  
  MOFAobject <- selectModel(MOFAlist, plotit = FALSE)
  mofa.result[[Y.index[i,j,k,l,m,n]+1]] <-  MOFAobject
  plotVarianceExplained(MOFAobject)
  latentZmofa <- MOFAobject@Expectations[["Z"]]
  
  clusterRef <- as.integer(Y.labels[[Y.index[i,j,k,l,m,n]+1]])
  
  cluster.index <- NbClust(data = latentZmofa, diss = NULL, distance = "euclidean", min.nc = 2, max.nc = 8, method = "kmeans")
  Mofa.cluster.index[[Y.index[i,j,k,l,m,n]+1]] <- extCriteria(cluster.index[["Best.partition"]], clusterRef, "all")
  
  clusterMofa <- kmeans(latentZmofa, k, nstart = 25)
  Mofa.real.index[[Y.index[i,j,k,l,m,n]+1]] <- extCriteria(clusterMofa$cluster, clusterRef, "all")

              
              
              
            }
          }
        }
      }
    }
  }
}


save.image("D:/dissertacao/10_11.RData")


```


```{r}

for (i in 1:number.of.test){
  print("--------------------------------")
  print(paste0("precision: ", Mofa.cluster.index[[i]]$precision))
  print(paste0("recall: ", Mofa.cluster.index[[i]]$recall))
  print(paste0("jaccard: ", Mofa.cluster.index[[i]]$jaccard))
  print(paste0("rand: ", Mofa.cluster.index[[i]]$rand))
  print(paste0("folkes_mallows: ", Mofa.cluster.index[[i]]$folkes_mallows))
}

for (i in 1:number.of.test){
  Mofa.real.index[[i]]$precision
  Mofa.real.index[[i]]$recall
  Mofa.real.index[[i]]$jaccard
  Mofa.real.index[[i]]$rand
  Mofa.real.index[[i]]$folkes_mallows
}

```






iClusterPlus

Note:
-the matrix used for functions are suposed to be samples on row and features on coluns, different from MOFA



```{r}
library(iClusterPlus)
library(GenomicRanges)
library(gplots)
library(lattice)

```

load("10_17_Y1.RData")

```{r}

library(iClusterPlus)

icluster.result <- vector("list", length = length(Y.samples))
#time.used <- vector("list", length = length(Y.samples))
index <- 1
for(i in 1:1){       #length(samples.size)
  for(j in 1:length(latents.size)){
    for(k in 1:length(cluster.size)){
      for(l in 1:length(cluster.std)){
        for(m in 1:1){            #length(views.size)
          for(n in 1:length(features.size)){      #length(features.size)
            if(features.size[[n]] > latents.size[[j]]){
if(index > 54 & n > 3){
 start.time <-  Sys.time()
  data <- Y.samples[[Y.index[i,j,k,l,m,n]]]
  file.dir <- paste("icluster/sample_",index,sep="")
  if (file.exists(file.dir)){
  } else {
    dir.create(file.dir)
    paste(file.dir,"/",sep="")
    for(o in 1:8){
      cv.fit = tune.iClusterPlus(cpus=12,dt1=t(data$view_1),dt2=t(data$view_2),dt3=t(data$view_3),
      type=c("gaussian","gaussian","gaussian"),K=o,n.lambda=35, scale.lambda=c(1,1,1),maxiter=20)
      save(cv.fit, file=paste(file.dir,"/","cv.fit.",i,j,k,l,m,n,"k.",o,".Rdata",sep=""))
    }
  }
  end.time <-  Sys.time()
  time.used[[index]] <- end.time - start.time
  
}
  index <- index + 1
  }
          }
        }
      }
    }
  }
}


bit <- 0
for( x in 1:1280){
  vec <- c()
  
  vec <- c(vec,time.used[x])
}



```

time
```{r}

time <- list()
for(n in 1:length(features.size)){
  time[[n]] <- list()
  for(j in 1:length(latents.size)){
    for(k in 1:cluster.size){
      for(l in 1:length(cluster.std)){
        if(features.size[[n]] > latents.size[[j]]){
          print(Y.index[1,j,k,l,1,n])
          time[[n]] <- c(time[[n]], time.used[[Y.index[1,j,k,l,1,n]]])
        }
      }
    }
  }
}



for(n in 1:length(features.size)){
    print(paste("---------------------------",n))
    vec <- unlist(time[[n]])
    #geom_bar(vec)
    print(table(vec))
    #print(paste("Cluster size:",latents.size[[k]], "Number of features : ", features.size[[n]]))
}

for(n in 1:length(features.size)){
    print(paste("---------------------------",n))
    vec <- unlist(time[[n]])
    if(n > 3){
      vec <- vec[2:length(vec)]
    }
    print(mean(vec))
    print(var(vec))
}




```







```{r}

iCluster.index.wss <- vector("list", length = length(Y.samples))

icluster.lf.found <- list()

index <- 1
for(i in 1:1){       #length(samples.size)
  for(j in 1:length(latents.size)){
    for(k in 1:length(cluster.size)){
      for(l in 1:length(cluster.std)){
        for(m in 1:1){            #length(views.size)
          for(n in 1:length(features.size)){      #length(features.size)
            if(features.size[[n]] > latents.size[[j]]){
              
if(index > 0 & n < 4){
  data <- Y.samples[[Y.index[i,j,k,l,m,n]]]
  file.dir <- paste("icluster/sample_",index,sep="")
  output=alist()
  pat <- paste("cv.fit.",i,j,k,l,m,n,"k.",sep="")
  files=grep(pat,dir(paste(file.dir,"/",sep="")))

  for(x  in 1:length(files)){
    load(paste(file.dir,"/",dir(paste(file.dir,"/",sep=""))[files[x]],sep=""))
    output[[x]]=cv.fit
  }
  
  nLambda = nrow(output[[1]]$lambda)
  nK = length(output)
  BIC = getBIC(output)
  devR = getDevR(output)
  
  minBICid = apply(BIC,2,which.min)
  devRatMinBIC = rep(NA,nK)
  for(x in 1:nK){
    devRatMinBIC[x] = devR[minBICid[x],x]
  }
  #plot(1:(nK+1),1 - c(0,devRatMinBIC),type="b",xlab="Number of clusters (K+1)", ylab="%Explained Variation")
  
  
  # Clusters = LF + 1，  K = number of clusters
  K <- find.k.by.second.derivative(1 - c(0,devRatMinBIC))
  #K <- find.k.by.max.dist(devRatMinBIC)
  
  icluster.lf.found[[Y.index[i,j,k,l,m,n]]] <- K
  
  clusters <- getClusters(output)
  
  rownames(clusters)=rownames(t(data[[1]]))
  colnames(clusters)=paste("K=",2:(length(output)+1),sep="")
  clusterRef <- as.integer(Y.labels[[Y.index[i,j,k,l,m,n]]]+1)
  iCluster.index.wss[[Y.index[i,j,k,l,m,n]]] <- extCriteria(clusters[,K], clusterRef, "all")
  
  
}
index <- index + 1




            }
          }
        }
      }
    }
  }
}
```




```{r}
find.k.by.second.derivative <- function(elbow.values){
  number.of.k <- length(elbow.values)
  first.derivative <- vector()
  for(i in 2:number.of.k - 1){
    first.derivative[[i+1]] <- elbow.values[[i]] - elbow.values[[i+1]]
  }
  second.derivative <- vector()
  for(i in 3:number.of.k-2){
    second.derivative[[i+2]] <- first.derivative[[i+1]] - first.derivative[[i+2]]
  }
  strength <- vector()
  relative.strength <- vector()
  
  
  dist_to_line <- vector()
  for(i in 1:number.of.k){
    dist_to_line[[i]] <- dist2d(c(i,elbow.values[[i]]), c(1,elbow.values[[1]]), c(number.of.k,elbow.values[[number.of.k]]))
  }
  
  
  check <- 0
  for(i in 3:number.of.k-1){
    if(second.derivative[[i+1]] > first.derivative[[i+1]]){
      strength[[i]] <- second.derivative[[i+1]] - first.derivative[[i+1]]
      relative.strength[[i]] <- strength[[i]]/(i)
      check <- 1
    }
  }
  
  if(check == 0){
    index <- which.max(dist_to_line)
  }else{
    index <- which.max(relative.strength)
  }
  
  
  return(index)
  
  
  
}


find.k.by.max.dist <- function(elbow.value){
  number.of.k <- length(elbow.values)
  
  dist_to_line <- vector()
  for(i in 1:number.of.k){
    dist_to_line[[i]] <- dist2d(c(i,elbow.values[[i]]), c(1,elbow.values[[1]]),c(number.of.k,elbow.values[[number.of.k]]))
  }
  
  index <- which.max(dist_to_line)
  
  return(index)
}


print.cluster.index.fig <- function(cluster.index){
  plot(cluster.std, cluster.index[,1],  type="o",  col="blue",  lty=1, ylim=c(0,1), ylab="Index Value" )
  points(cluster.std, cluster.index[,2], col="red")
  lines(cluster.std, cluster.index[,2], col="red",lty=2)
  points(cluster.std, cluster.index[,3], col="green")
  lines(cluster.std, cluster.index[,3], col="green",lty=3)
  points(cluster.std, cluster.index[,4], col="yellow")
  lines(cluster.std, cluster.index[,4], col="yellow",lty=4)
  points(cluster.std, cluster.index[,5], col="gray")
  lines(cluster.std, cluster.index[,5], col="gray",lty=5)
  legend(0.3,0.5,legend=c("precision","recall","jaccard","rand","folkes_mallows"), col=c("blue","red","green","yellow","gray"), lty=c(1,2,3,4,5), ncol=1)
}


dist2d <- function(a,b,c) {
 v1 <- b - c
 v2 <- a - b
 m1 <- cbind(v1,v2)
 d <- abs(det(m1))/sqrt(sum(v1*v1))
} 

```


```{r}
wss.index <- matrix(0L, nrow = length(cluster.std), ncol = 5)

colnames(wss.index) <- c("precision","recall","jaccard","rand","folkes_mallows")
rownames(wss.index) <- paste0("std=", cluster.std[1:length(cluster.std)])

for(j in 1:length(latents.size)){
  for(n in 1:3){
    for(k in 1:length(cluster.size)){
      for(a in 1:length(cluster.std)){
        wss <- iCluster.index.wss[[Y.index[1,j,k,a,1,n]]]
        
          wss.index[a,1] <- wss.index[a,1] + wss$precision
          wss.index[a,2] <- wss.index[a,2] + wss$recall
          wss.index[a,3] <- wss.index[a,3] + wss$jaccard
          wss.index[a,4] <- wss.index[a,4] + wss$rand
          wss.index[a,5] <- wss.index[a,5] + wss$folkes_mallows
      }
    }
  }
}




print.cluster.index.fig(wss.index/(4*3*8))
```




```{r}
features.lf <- list()


for(j in 1:length(latents.size)){
  features.lf[[j]] <- list()
  for(n in 1:3){
    features.lf[[j]][[n]] <-list()
    for(k in 1:length(cluster.size)){
      for(l in 1:length(cluster.std)){
        features.lf[[j]][[n]] <- c(features.lf[[j]][[n]], icluster.lf.found[[Y.index[1,j,k,l,1,n]]])
        
      }
    }
  }
}

for(k in 1:length(latents.size)){
  for(n in 1:3){
    vec <- unlist(features.lf[[k]][[n]])
    #geom_bar(vec)
    print(table(vec))
    print("----------------")
    print(paste("Cluster size:",latents.size[[k]], "Number of features : ", features.size[[n]]))
}}

```






```{r}
output=alist()

pat <- paste("cv.fit.",i,j,k,l,m,n,"k.",sep="")
#files=grep("cv.fit",dir())
files=grep(pat,dir(paste(file.dir,"/",sep="")))

for(i in 1:length(files)){
  load(paste(file.dir,"/",dir(paste(file.dir,"/",sep=""))[files[i]],sep=""))
  output[[i]]=cv.fit
}

nLambda = nrow(output[[1]]$lambda)
nK = length(output)
BIC = getBIC(output)
devR = getDevR(output)

minBICid = apply(BIC,2,which.min)
devRatMinBIC = rep(NA,nK)
for(i in 1:nK){
  devRatMinBIC[i] = devR[minBICid[i],i]
}
plot(1:(nK+1),c(0,devRatMinBIC),type="b",xlab="Number of clusters (K+1)", ylab="%Explained Variation")

ncol(BIC)
```




```{r}

output=alist()

files=grep("cv.fit",dir())

for(i in 1:length(files)){
  load(dir()[files[i]])
  output[[i]]=cv.fit
  
}

nLambda = nrow(output[[1]]$lambda)
nK = length(output)

BIC = getBIC(output)
devR = getDevR(output)



```




```{r}

minBICid = apply(BIC,2,which.min)

devRatMinBIC = rep(NA,nK)

for(i in 1:nK){
  devRatMinBIC[i] = devR[minBICid[i],i]
}




```


```{r}

plot(1:(nK+1),c(0,devRatMinBIC),type="b",xlab="Number of clusters (K+1)", ylab="%Explained Variation")

```



```{r}

clusters=getClusters(output)
rownames(clusters)=rownames(t(simulated.data$view_1))
colnames(clusters)=paste("K=",2:(length(output)+1),sep="")




#write.table(clusters, file="clusterMembership.txt",sep='\t',quote=F)

k=3
best.cluster=clusters[,k]
best.fit=output[[k]]$fit[[which.min(BIC[,k])]]

clusterCrteria <- extCriteria(best.cluster, clusterRef, "all")
print("Precision:",clusterCrteria$precision)
clusterCrteria$recall
clusterCrteria$jaccard
clusterCrteria$rand
clusterCrteria$folkes_mallows

best.lantent <- best.fit$meanZ
rownames(best.lantent) <- rownames(t(simulated.data$view_1))
for(feature in 1:dim(best.lantent)[2]){
  print(paste("feature number:", feature))
  den <- density(best.lantent[,feature])
  plot(den)
}



```






```{r}
W <- best.fit$beta

for (l in 1:length(W)){
  rownames(W[[l]]) <- rownames(simulated.data$view_1)
  colnames(W[[l]]) <- c("lf1","lf2","lf3","lf4","lf5")
}




#lista com pesos de cada variave em difenrentes view na latent variable (nao funciona quando tem features difenrentes em cada view)
fw <- list()
for (l in 1:dim(W[[1]])[2]){
  fw[[l]] <- matrix(data = 1:dim(W[[1]])[1], nrow = dim(W[[1]])[1], ncol = 1)  
  for (v in 1:length(W)){
    fw[[l]] <- cbind(fw[[l]], W[[v]][,l])
  }
  fw[[l]]<-fw[[l]][,-1]
  rownames(fw[[l]]) <- rownames(simulated.data$view_1)
}


#lista com pesos de cada lf em cada view (cumprimento total=v*f)
fwl <- list()

i <- 0
for (l in 1:dim(W[[1]])[2]){
  for (v in 1:length(W)){
    i <- i + 1 
    fwl[[i]] <- W[[v]][,l]
  }
}


#remove 0 weight features
fwl <- lapply(fwl, function(x) {x[x!=0]})

#sort feature weights
fwl <- lapply(fwl, function(x) {x[order(abs(x), decreasing = TRUE)]})



fwl[[1]][order(abs(fwl[[1]]), decreasing = TRUE)]








W1 <- best.fit$beta[[1]]
heatmap(W1)



```




Cluster result figure

```{r}
library(condformat)


smoke <- matrix(c(51,43,22,92,28,21,68,22,9),ncol=3,byrow=TRUE)
colnames(smoke) <- c("High","Low","Middle")
rownames(smoke) <- c("current","former","never")
smoke <- as.table(smoke)
smoke

```


